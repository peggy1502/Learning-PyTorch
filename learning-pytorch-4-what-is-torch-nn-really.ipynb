{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Learning PyTorch: What is `torch.nn` really?\n\nSource: https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n(*by Jeremy Howard, fast.ai. Thanks to Rachel Thomas and Francisco Ingham*)\n\nPyTorch provides the elegantly designed modules and classes `torch.nn` , `torch.optim` , `Dataset` , and `DataLoader` to help us create and train neural networks. The aim here is to really understand exactly what they’re doing.\n\nWe will first train basic neural net on the MNIST data set without using any features from these models. Then, we will incrementally add one feature from `torch.nn`, `torch.optim`, `Dataset`, or `DataLoader` at a time, showing exactly what each piece does, and how it works to make the code either more concise, or more flexible.\n\n# MNIST data setup\n\nWe will use the classic MNIST dataset, which consists of black-and-white images of hand-drawn digits (between 0 and 9).\n\nThis dataset is in numpy array format, and has been stored using pickle, a python-specific format for serializing data.\n\nEach image is 28 x 28, and is being stored as a flattened row of length 784 (=28x28).","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport requests           # to download the dataset\n\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:28.177062Z","iopub.execute_input":"2021-08-09T13:41:28.177722Z","iopub.status.idle":"2021-08-09T13:41:29.741397Z","shell.execute_reply.started":"2021-08-09T13:41:28.177631Z","shell.execute_reply":"2021-08-09T13:41:29.740372Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport gzip\n\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:29.742948Z","iopub.execute_input":"2021-08-09T13:41:29.743256Z","iopub.status.idle":"2021-08-09T13:41:30.953177Z","shell.execute_reply.started":"2021-08-09T13:41:29.743228Z","shell.execute_reply":"2021-08-09T13:41:30.952140Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\nimport numpy as np\n\n# Let's take a look at one of the image.\n# We need to reshape it to 2d first.\n\npyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\nprint(\"Shape of x_train:\", x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:30.955008Z","iopub.execute_input":"2021-08-09T13:41:30.955290Z","iopub.status.idle":"2021-08-09T13:41:31.154165Z","shell.execute_reply.started":"2021-08-09T13:41:30.955263Z","shell.execute_reply":"2021-08-09T13:41:31.153012Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Shape of x_train: (50000, 784)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# Let's take a look at the data representing the above image.\nx_train[0]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-09T13:41:31.156252Z","iopub.execute_input":"2021-08-09T13:41:31.156733Z","iopub.status.idle":"2021-08-09T13:41:31.181828Z","shell.execute_reply.started":"2021-08-09T13:41:31.156685Z","shell.execute_reply":"2021-08-09T13:41:31.180058Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"array([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.01171875, 0.0703125 , 0.0703125 ,\n       0.0703125 , 0.4921875 , 0.53125   , 0.68359375, 0.1015625 ,\n       0.6484375 , 0.99609375, 0.96484375, 0.49609375, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.1171875 , 0.140625  , 0.3671875 , 0.6015625 ,\n       0.6640625 , 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n       0.98828125, 0.87890625, 0.671875  , 0.98828125, 0.9453125 ,\n       0.76171875, 0.25      , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.19140625, 0.9296875 ,\n       0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n       0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.36328125,\n       0.3203125 , 0.3203125 , 0.21875   , 0.15234375, 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.0703125 , 0.85546875, 0.98828125, 0.98828125,\n       0.98828125, 0.98828125, 0.98828125, 0.7734375 , 0.7109375 ,\n       0.96484375, 0.94140625, 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.3125    , 0.609375  , 0.41796875, 0.98828125, 0.98828125,\n       0.80078125, 0.04296875, 0.        , 0.16796875, 0.6015625 ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.0546875 ,\n       0.00390625, 0.6015625 , 0.98828125, 0.3515625 , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.54296875,\n       0.98828125, 0.7421875 , 0.0078125 , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.04296875, 0.7421875 , 0.98828125,\n       0.2734375 , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.13671875, 0.94140625, 0.87890625, 0.625     ,\n       0.421875  , 0.00390625, 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.31640625, 0.9375    , 0.98828125, 0.98828125, 0.46484375,\n       0.09765625, 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.17578125,\n       0.7265625 , 0.98828125, 0.98828125, 0.5859375 , 0.10546875,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.0625    , 0.36328125,\n       0.984375  , 0.98828125, 0.73046875, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.97265625, 0.98828125,\n       0.97265625, 0.25      , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.1796875 , 0.5078125 ,\n       0.71484375, 0.98828125, 0.98828125, 0.80859375, 0.0078125 ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.15234375,\n       0.578125  , 0.89453125, 0.98828125, 0.98828125, 0.98828125,\n       0.9765625 , 0.7109375 , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.09375   , 0.4453125 , 0.86328125, 0.98828125, 0.98828125,\n       0.98828125, 0.98828125, 0.78515625, 0.3046875 , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.08984375, 0.2578125 , 0.83203125, 0.98828125,\n       0.98828125, 0.98828125, 0.98828125, 0.7734375 , 0.31640625,\n       0.0078125 , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.0703125 , 0.66796875, 0.85546875,\n       0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.76171875,\n       0.3125    , 0.03515625, 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.21484375, 0.671875  ,\n       0.8828125 , 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n       0.953125  , 0.51953125, 0.04296875, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.53125   , 0.98828125, 0.98828125, 0.98828125,\n       0.828125  , 0.52734375, 0.515625  , 0.0625    , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        ], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# PyTorch uses torch.tensor, rather than numpy arrays, so we need to convert our data.\n\nimport torch\n\nx_train, y_train, x_valid, y_valid = map(torch.tensor, \n                                         (x_train, y_train, x_valid, y_valid)\n                                         )\n\nn, c = x_train.shape                # n=total records (50000), c=784\n\nprint(\"== x_train ==\")\nprint(x_train)\nprint(x_train.shape)\n\nprint(\"\\n== y_train ==\")\nprint(y_train)\nprint(y_train.shape)\nprint(y_train.min(), y_train.max()) # target contains values from 0 to 9","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:31.183693Z","iopub.execute_input":"2021-08-09T13:41:31.184871Z","iopub.status.idle":"2021-08-09T13:41:32.455711Z","shell.execute_reply.started":"2021-08-09T13:41:31.184813Z","shell.execute_reply":"2021-08-09T13:41:32.454307Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"== x_train ==\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\ntorch.Size([50000, 784])\n\n== y_train ==\ntensor([5, 0, 4,  ..., 8, 4, 8])\ntorch.Size([50000])\ntensor(0) tensor(9)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1. Neural net from scratch (no torch.nn)\n\nWe will create and train a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch.\n\nFor this simple linear model, we will create random tensors for weights and zero-filled tensors for bias.\n\nWe tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n\nWe are initializing the weights here with [Xavier initialisation](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) (by multiplying with `1/sqrt(n)`).\n\nFor the weights, we set `requires_grad` after the initialization, since we don’t want that step included in the gradient. (Note that a trailing `_` in PyTorch signifies that the operation is performed in-place.)","metadata":{}},{"cell_type":"code","source":"import math\n\nweights = torch.randn(784, 10) / math.sqrt(784)\nweights.requires_grad_()\nbias = torch.zeros(10, requires_grad=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.457842Z","iopub.execute_input":"2021-08-09T13:41:32.458314Z","iopub.status.idle":"2021-08-09T13:41:32.479494Z","shell.execute_reply.started":"2021-08-09T13:41:32.458279Z","shell.execute_reply":"2021-08-09T13:41:32.477978Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"weights  # Take a look at the weights","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.482652Z","iopub.execute_input":"2021-08-09T13:41:32.483320Z","iopub.status.idle":"2021-08-09T13:41:32.505891Z","shell.execute_reply.started":"2021-08-09T13:41:32.483269Z","shell.execute_reply":"2021-08-09T13:41:32.504761Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0281, -0.0047,  0.0365,  ..., -0.0185,  0.0296, -0.0136],\n        [-0.0302,  0.0453, -0.0247,  ..., -0.0172,  0.0381, -0.0511],\n        [ 0.0258, -0.0097,  0.0454,  ...,  0.0005,  0.0536,  0.1089],\n        ...,\n        [-0.0355,  0.0197, -0.0262,  ...,  0.0171, -0.0237,  0.0482],\n        [ 0.0014,  0.0013, -0.0051,  ..., -0.0300, -0.0360, -0.0413],\n        [-0.0104,  0.0562,  0.0538,  ...,  0.0528, -0.0199, -0.0098]],\n       requires_grad=True)"},"metadata":{}}]},{"cell_type":"code","source":"bias   # Take a look at the bias","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.510520Z","iopub.execute_input":"2021-08-09T13:41:32.511096Z","iopub.status.idle":"2021-08-09T13:41:32.526456Z","shell.execute_reply.started":"2021-08-09T13:41:32.511041Z","shell.execute_reply":"2021-08-09T13:41:32.524912Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Model and Activation Function\n\nThanks to PyTorch’s ability to calculate gradients automatically, we can use any standard Python function (or callable object) as a model! \n\nSo let’s just write a plain matrix multiplication and broadcasted addition to create a simple linear model. We also need an activation function, so we’ll write `log_softmax` and use it. PyTorch will even create fast GPU or vectorized CPU code for your function automatically.\n\nNote: \n\nThe [discussion here](\nhttps://discuss.pytorch.org/t/what-is-the-difference-between-log-softmax-and-softmax/11801) mentioned that `log_softmax` essentially does `log(softmax(x))`, but the practical implementation is different and more efficient while doing the same operation.\n\nAnd the answer provided [here](https://datascience.stackexchange.com/questions/40714/what-is-the-advantage-of-using-log-softmax-instead-of-softmax) explains the formula used in the function below.\n\nWe can also read [here](https://zhang-yang.medium.com/understanding-cross-entropy-implementation-in-pytorch-softmax-log-softmax-nll-cross-entropy-416a2b200e34) on how Pytorch’s Cross Entropy function relates to softmax, log softmax, and NLL.","metadata":{}},{"cell_type":"code","source":"def log_softmax(x):\n    # Seems like this is returning the log likelihood?\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model(xb):\n    return log_softmax(xb @ weights + bias) ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.529022Z","iopub.execute_input":"2021-08-09T13:41:32.529484Z","iopub.status.idle":"2021-08-09T13:41:32.536201Z","shell.execute_reply.started":"2021-08-09T13:41:32.529427Z","shell.execute_reply":"2021-08-09T13:41:32.535236Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# We will call our function on one batch of data (in this case, 64 images). \n# This is one forward pass. \n\nbs = 64             # batch size\n\nxb = x_train[0:bs]  # a mini-batch from x, with shape [64, 784]\npreds = model(xb)   # predictions from 1 batch of data\n\n# Print values of the first 5 predictions in the batch\nfor i in range(5):\n    print(\"preds[%d]:\" %i, preds[i])        \nprint(\"\\nShape of preds:\", preds.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.537365Z","iopub.execute_input":"2021-08-09T13:41:32.537836Z","iopub.status.idle":"2021-08-09T13:41:32.608265Z","shell.execute_reply.started":"2021-08-09T13:41:32.537794Z","shell.execute_reply":"2021-08-09T13:41:32.606334Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"preds[0]: tensor([-2.5770, -2.3581, -2.4281, -2.8149, -1.7359, -2.3735, -2.1032, -2.5785,\n        -2.0805, -2.4185], grad_fn=<SelectBackward>)\npreds[1]: tensor([-2.5676, -2.5730, -2.7850, -2.8245, -2.2423, -2.0983, -1.4850, -2.6982,\n        -2.3245, -2.2504], grad_fn=<SelectBackward>)\npreds[2]: tensor([-1.8988, -2.8607, -2.3521, -2.6710, -2.5738, -2.0666, -2.5532, -2.2485,\n        -2.0314, -2.1959], grad_fn=<SelectBackward>)\npreds[3]: tensor([-2.2147, -2.2699, -2.2328, -2.8628, -2.2602, -2.6489, -1.7395, -2.8289,\n        -2.1399, -2.3462], grad_fn=<SelectBackward>)\npreds[4]: tensor([-1.5505, -2.7288, -2.8154, -3.3499, -2.0239, -2.5469, -1.8582, -2.4600,\n        -2.5191, -2.3518], grad_fn=<SelectBackward>)\n\nShape of preds: torch.Size([64, 10])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's take a look at (xb @ weights + bias), i.e. the data passed into log_softmax().\n\n# shape of xb     : [64, 784]\n# shape of weights: [784, 10]\n# shape of bias   : [10]\n    \n(xb @ weights + bias) # shape is [64, 10]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-09T13:41:32.610265Z","iopub.execute_input":"2021-08-09T13:41:32.610846Z","iopub.status.idle":"2021-08-09T13:41:32.629689Z","shell.execute_reply.started":"2021-08-09T13:41:32.610768Z","shell.execute_reply":"2021-08-09T13:41:32.628635Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor([[-1.8335e-01,  3.5551e-02, -3.4449e-02, -4.2131e-01,  6.5773e-01,\n          2.0164e-02,  2.9042e-01, -1.8486e-01,  3.1317e-01, -2.4823e-02],\n        [-3.5540e-02, -4.0974e-02, -2.5302e-01, -2.9251e-01,  2.8976e-01,\n          4.3371e-01,  1.0470e+00, -1.6615e-01,  2.0756e-01,  2.8165e-01],\n        [ 5.4016e-01, -4.2169e-01,  8.6883e-02, -2.3198e-01, -1.3482e-01,\n          3.7242e-01, -1.1419e-01,  1.9047e-01,  4.0756e-01,  2.4309e-01],\n        [ 9.4622e-02,  3.9392e-02,  7.6493e-02, -5.5348e-01,  4.9136e-02,\n         -3.3965e-01,  5.6979e-01, -5.1965e-01,  1.6942e-01, -3.6889e-02],\n        [ 9.2329e-01, -2.5501e-01, -3.4165e-01, -8.7608e-01,  4.4992e-01,\n         -7.3117e-02,  6.1553e-01,  1.3807e-02, -4.5347e-02,  1.2200e-01],\n        [ 3.4730e-01, -3.9007e-01, -1.6095e-01, -5.2212e-01,  3.4346e-01,\n          1.9819e-02,  5.6218e-01, -4.3901e-01,  6.3256e-02,  1.0479e-02],\n        [-2.0256e-02, -6.7246e-02, -2.6784e-01, -8.3665e-02,  4.5763e-01,\n         -2.5513e-01,  4.2428e-01,  2.6996e-01, -1.3806e-01,  3.9007e-02],\n        [ 5.0730e-01, -4.5382e-01,  1.7195e-01, -4.7660e-01,  5.5313e-01,\n          1.5477e-01,  5.8507e-01, -4.5343e-01,  3.9515e-01, -1.3646e-01],\n        [-1.6642e-01, -1.4839e-01,  2.1621e-02, -4.2202e-01,  4.0790e-01,\n         -1.2196e-01,  1.9599e-01,  1.1280e-01, -2.5306e-02, -1.0631e-01],\n        [ 2.3084e-01, -9.5045e-02, -1.0899e-01, -2.9179e-01,  4.5302e-01,\n          1.1370e-01,  3.6627e-01, -4.4532e-01,  2.6765e-01,  9.4976e-02],\n        [-1.4150e-01, -5.8837e-02, -3.3780e-01, -6.0921e-01,  5.0546e-01,\n         -3.9846e-01,  4.5039e-01,  9.9975e-02, -2.2992e-01, -1.7673e-01],\n        [ 2.3122e-01, -3.4914e-01,  5.9410e-02, -4.9607e-01,  3.1589e-01,\n         -3.1345e-02,  5.2046e-01, -1.8258e-01,  5.1899e-02,  4.3809e-01],\n        [ 2.9831e-01, -6.0941e-01, -3.1237e-01, -1.8964e-01,  4.7590e-01,\n         -3.7714e-01,  3.4299e-01,  6.2111e-01, -2.6783e-01, -4.2817e-01],\n        [-1.3075e-01, -2.2593e-01, -3.5748e-01, -5.2058e-01,  7.4098e-02,\n          6.0615e-01,  4.0340e-01, -3.1700e-01,  5.0841e-01,  4.0230e-01],\n        [-2.3387e-01, -2.7331e-01, -8.2427e-02, -2.4967e-01,  2.9360e-01,\n         -2.5694e-01,  1.0059e-03,  3.5145e-02,  2.1870e-01, -1.0257e-02],\n        [ 2.9984e-01, -2.5512e-01, -4.3158e-01, -2.9644e-01,  2.7117e-01,\n          3.8954e-01,  5.4609e-01, -5.3233e-01,  2.5205e-01, -7.8182e-02],\n        [ 6.9634e-02, -2.5809e-01, -2.6275e-02, -2.8032e-01,  3.0688e-01,\n         -1.2231e-01,  3.2889e-01, -2.4211e-01,  5.7036e-01,  1.7324e-01],\n        [ 3.0666e-01, -3.7575e-01,  3.8138e-02, -4.8816e-01,  6.1421e-01,\n         -9.3388e-02,  9.8852e-01, -5.6188e-01,  9.4477e-03,  1.1393e-01],\n        [ 1.7259e-01, -2.9242e-01, -3.0892e-02,  2.9163e-01,  6.7415e-02,\n         -2.0497e-01,  1.3158e-01, -2.8007e-01,  2.9763e-01,  8.9132e-02],\n        [ 3.8976e-02, -1.8264e-01, -2.0717e-01, -2.7446e-01,  6.3273e-01,\n          1.4589e-02,  6.0429e-01, -3.1442e-01,  1.3783e-01, -1.4393e-01],\n        [ 9.6760e-01, -5.0076e-01,  9.7478e-02, -6.6452e-01,  1.8012e-01,\n          3.4412e-02, -1.2423e-02,  3.0209e-01,  1.9381e-01,  8.6386e-01],\n        [ 2.8255e-01,  1.1353e-01, -3.2261e-01, -4.7280e-01,  3.9575e-01,\n          1.6714e-01,  1.0636e+00, -4.2467e-03, -6.7175e-02,  2.6139e-01],\n        [ 3.3082e-01, -3.7762e-02, -6.2439e-02, -1.0202e-01,  3.7590e-01,\n          1.4711e-01,  2.7306e-01, -5.4392e-01,  3.9919e-01,  2.1921e-01],\n        [ 1.6796e-01,  8.4971e-02,  1.2219e-01, -5.9596e-01,  1.0477e-01,\n         -3.3207e-01,  5.8339e-01, -5.0545e-01,  1.1290e-01,  2.2487e-02],\n        [ 4.2787e-01, -2.3107e-01, -4.9878e-01, -3.5051e-01,  4.5816e-01,\n         -2.2410e-01,  6.9057e-01,  4.5955e-02, -8.6812e-02,  5.7024e-02],\n        [ 9.1359e-01, -4.9893e-01,  1.3305e-01, -3.9814e-01,  4.4059e-01,\n          1.1337e-01,  5.6299e-01, -1.6434e-01,  1.5358e-01, -4.6953e-02],\n        [ 2.2531e-01,  2.5867e-02, -2.0000e-01, -8.1704e-01,  1.6935e-01,\n         -2.4910e-01,  2.0101e-01,  1.5427e-01, -1.6600e-01, -1.1082e-01],\n        [ 3.5455e-01, -4.5543e-01, -1.0474e-01, -4.5811e-01,  1.0441e+00,\n         -1.2237e-01,  5.8875e-01, -1.2819e-01,  1.5483e-01, -3.1833e-01],\n        [ 1.8029e-01, -5.7450e-01, -2.8613e-01, -2.0005e-01,  5.6836e-01,\n          2.3787e-01,  7.0486e-02, -2.0398e-01, -1.9872e-01, -6.8134e-02],\n        [ 1.8569e-01, -8.4972e-02, -1.9406e-01, -6.2815e-01,  1.6920e-01,\n         -3.5956e-01,  2.7230e-01, -4.0683e-01,  4.0900e-02, -3.7792e-02],\n        [ 3.0800e-01, -1.0968e-01, -5.6966e-01, -2.9072e-01,  2.5885e-01,\n         -3.3996e-01, -1.1812e-02,  3.8304e-01, -4.7032e-01,  5.8795e-01],\n        [ 2.0347e-02, -4.7504e-01,  2.7813e-01, -4.1734e-01,  3.8435e-01,\n          1.7024e-01,  1.0407e+00, -6.1318e-01, -1.5720e-03,  5.9924e-02],\n        [ 2.1182e-01, -6.7905e-02, -1.8917e-01, -9.8940e-02,  3.2737e-01,\n          8.3675e-02, -1.0678e-01, -9.2718e-02,  6.6724e-01,  1.6478e-01],\n        [ 4.7987e-01, -4.1579e-02, -2.2535e-01, -3.9594e-01,  6.6577e-01,\n          1.2763e-01,  5.8464e-01, -6.4074e-01,  2.5228e-01, -5.2819e-02],\n        [ 5.3105e-02, -3.1039e-01, -1.3258e-01, -1.0165e+00,  4.7267e-01,\n          4.3622e-01,  7.5674e-01, -4.0963e-01,  2.6468e-01, -1.7314e-02],\n        [ 3.0096e-01, -3.4166e-02, -3.4629e-01, -3.8620e-01,  6.8136e-01,\n         -1.5537e-01,  5.7604e-01, -5.4054e-02, -2.4883e-01, -2.2840e-01],\n        [ 1.2310e-01, -5.1344e-01, -2.4953e-01, -1.8528e-01,  4.8975e-01,\n         -1.7758e-01,  1.7382e-01,  3.8896e-01,  5.2436e-01,  3.4115e-01],\n        [ 4.5610e-01, -3.0045e-01, -1.2473e-01, -6.8791e-01,  5.2664e-01,\n          5.7864e-01,  1.0987e+00, -4.4230e-01,  1.3136e-01,  3.4750e-01],\n        [ 2.2851e-01, -7.1054e-03, -5.1951e-01, -4.7488e-01,  5.0721e-01,\n         -1.5667e-01,  2.7298e-01,  6.2023e-03, -3.1142e-01,  4.4571e-01],\n        [ 9.0534e-02, -4.0873e-01, -3.2615e-01, -4.1264e-01,  5.5553e-01,\n          2.5444e-01,  3.2638e-02, -5.1872e-01,  4.7404e-01,  2.4510e-01],\n        [-8.0157e-02, -8.9955e-02, -1.8335e-02, -3.3451e-01,  2.5124e-01,\n         -2.9819e-01,  2.7721e-01, -2.5050e-02, -9.6652e-03,  1.2802e-01],\n        [ 3.6298e-01,  1.5017e-01, -9.8587e-02, -9.3796e-01,  5.5320e-01,\n         -4.7613e-01,  7.6558e-01, -4.0798e-01, -1.1335e-01, -1.2620e-01],\n        [ 3.4809e-01,  1.2128e-01, -4.0584e-01, -3.5650e-01,  5.0665e-01,\n         -1.0027e-01,  1.9504e-01,  1.4004e-02, -2.7034e-01, -5.9015e-02],\n        [ 4.7573e-02, -5.8950e-02, -2.7990e-01, -4.1660e-01,  3.5982e-01,\n         -2.6647e-01,  5.3014e-01, -5.9900e-02, -6.1246e-02,  1.5701e-01],\n        [ 2.2571e-01, -4.1719e-01, -2.5054e-01, -3.4063e-01,  3.0706e-01,\n         -2.4594e-01,  4.1162e-01,  1.1595e-01, -4.2904e-02,  9.4686e-02],\n        [ 6.0316e-01, -2.7777e-01, -5.8008e-01, -7.0340e-01,  4.4816e-01,\n         -1.2627e-01,  3.4943e-01, -1.2130e-01, -2.3864e-01,  2.1568e-01],\n        [ 3.0711e-01, -1.1984e-01, -2.1250e-01, -6.0201e-01,  4.1506e-01,\n         -2.8163e-01,  4.5496e-01, -3.2634e-01, -6.0798e-02, -2.6183e-02],\n        [ 3.4580e-01, -1.7717e-01,  2.1070e-01, -3.3729e-01,  6.6958e-01,\n          2.6120e-01,  1.0961e+00,  8.1773e-02, -9.5209e-04,  5.5992e-02],\n        [ 3.6516e-01,  2.5132e-01, -5.0145e-01, -3.0718e-01,  4.1362e-01,\n         -3.3306e-01,  2.1828e-01, -1.9472e-02,  2.5237e-02,  1.5058e-02],\n        [ 1.8093e-01, -4.7130e-02, -1.2275e-01, -7.1388e-01,  5.9080e-01,\n         -4.1296e-01,  9.0174e-01, -6.7352e-02,  3.6278e-01, -1.0151e-01],\n        [ 1.5630e-01, -3.1754e-01, -1.0117e-01, -1.6957e-02,  4.8579e-01,\n         -6.6732e-02,  1.9616e-01,  2.1372e-01,  3.0182e-01, -2.7131e-01],\n        [ 1.9227e-01, -1.7078e-01,  9.0188e-02, -9.9638e-02,  6.3936e-01,\n          2.8878e-01,  7.2388e-01,  2.1630e-02, -2.0974e-01,  9.5141e-02],\n        [ 1.1246e-01, -6.3054e-02, -6.5892e-01, -7.4500e-01,  4.4710e-01,\n          2.3615e-01, -1.2348e-01,  3.8988e-02, -2.7522e-01,  2.6220e-01],\n        [ 1.1279e-01, -3.5991e-02, -2.0310e-01, -2.1559e-01,  5.7257e-02,\n         -1.4336e-01,  2.0987e-01, -2.4099e-01,  1.5749e-01,  1.0670e-01],\n        [ 1.3615e-01, -2.5870e-01, -2.4651e-01, -8.9291e-01,  2.7865e-01,\n          8.1151e-02, -1.5178e-02, -1.4538e-01, -3.4853e-01,  8.5209e-02],\n        [ 2.8619e-01, -1.5539e-01,  8.9339e-02, -1.1442e+00,  7.6523e-01,\n         -1.8648e-01,  6.9766e-01, -4.0359e-01, -2.7657e-01,  1.2228e-03],\n        [ 4.4482e-01, -3.3937e-02, -2.3573e-01, -3.0897e-01,  3.3680e-01,\n          2.0190e-01,  4.1471e-01, -3.0304e-01, -9.9941e-02,  1.7859e-01],\n        [ 4.0303e-01, -3.0648e-01, -4.3243e-01, -3.3433e-01,  4.1785e-01,\n          1.9188e-01,  4.1169e-01, -5.4029e-01,  1.8915e-01,  1.5437e-01],\n        [ 9.7939e-01, -2.0097e-01, -2.6688e-01, -7.7159e-01,  3.7102e-01,\n         -4.4104e-02,  2.8838e-01,  6.2889e-01, -7.3183e-02,  5.2625e-01],\n        [ 2.0425e-01,  1.1843e-01, -3.4626e-02, -5.8640e-01,  1.2095e-01,\n         -3.4468e-01,  5.1318e-01, -2.8186e-01,  1.1362e-01, -1.1156e-01],\n        [ 2.3442e-01, -1.4483e-01, -1.1862e-02, -3.4215e-01,  9.6574e-02,\n          1.1603e-01,  2.6681e-02,  1.0442e-02,  1.6430e-01,  7.9249e-01],\n        [ 3.3311e-01, -1.6583e-01, -5.7914e-02, -4.2526e-01,  1.7350e-01,\n         -1.7103e-01,  3.6569e-01, -3.7179e-01,  1.9800e-01,  3.2311e-01],\n        [ 2.6485e-01, -3.8621e-01, -1.9986e-01, -3.2007e-01,  1.4329e-01,\n         -7.8709e-02,  8.3070e-02, -3.1274e-01,  4.2300e-01,  6.0840e-01],\n        [ 4.4006e-01, -1.0866e-01, -2.2080e-01, -2.8522e-01,  8.0869e-01,\n         -3.0774e-02,  1.1141e+00, -5.3434e-01, -2.8904e-01,  4.0702e-01]],\n       grad_fn=<AddBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Loss Function\n\nLet’s implement [negative log-likelihood](https://medium.com/deeplearningmadeeasy/negative-log-likelihood-6bd79b55d8b6) to use as the loss function.","metadata":{}},{"cell_type":"code","source":"def nll(preds, targets):\n    return -preds[range(targets.shape[0]), targets].mean()\n    \nloss_func = nll","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.631224Z","iopub.execute_input":"2021-08-09T13:41:32.631651Z","iopub.status.idle":"2021-08-09T13:41:32.637238Z","shell.execute_reply.started":"2021-08-09T13:41:32.631607Z","shell.execute_reply":"2021-08-09T13:41:32.636063Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"yb = y_train[0:bs]  # yb is the targets on a batch.\nprint(yb)           # yb contains 64 digits (value from 0 to 9).\nprint(\"\\nShape of yb:\" , yb.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.638941Z","iopub.execute_input":"2021-08-09T13:41:32.639388Z","iopub.status.idle":"2021-08-09T13:41:32.654273Z","shell.execute_reply.started":"2021-08-09T13:41:32.639327Z","shell.execute_reply":"2021-08-09T13:41:32.652775Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])\n\nShape of yb: torch.Size([64])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calling the loss function to calculate loss of the batch.\n\nprint(\"Loss:\", loss_func(preds, yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.656157Z","iopub.execute_input":"2021-08-09T13:41:32.656581Z","iopub.status.idle":"2021-08-09T13:41:32.668890Z","shell.execute_reply.started":"2021-08-09T13:41:32.656536Z","shell.execute_reply":"2021-08-09T13:41:32.668023Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Loss: tensor(2.4044, grad_fn=<NegBackward>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cross checking that the loss function is returning values based on the following:\n\nprint(-preds[range(64), yb])\nprint(\"\\nLoss:\", -preds[range(64), yb].mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.670213Z","iopub.execute_input":"2021-08-09T13:41:32.670721Z","iopub.status.idle":"2021-08-09T13:41:32.683870Z","shell.execute_reply.started":"2021-08-09T13:41:32.670689Z","shell.execute_reply":"2021-08-09T13:41:32.681922Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor([2.3735, 2.5676, 2.5738, 2.2699, 2.3518, 2.5068, 2.4379, 2.9445, 2.4495,\n        1.9445, 2.8817, 2.4378, 2.5346, 2.0181, 2.5390, 2.9147, 2.4197, 2.4624,\n        2.2163, 2.5343, 2.3859, 2.2502, 2.2181, 2.2489, 2.6323, 2.3776, 2.0961,\n        2.9342, 2.5894, 2.6417, 2.6381, 2.4619, 2.5330, 2.5111, 2.3679, 2.5385,\n        2.2746, 2.1376, 2.3543, 2.3365, 2.3909, 2.4972, 2.3289, 2.1807, 2.6662,\n        2.1280, 2.3718, 2.3478, 2.3422, 3.1763, 2.4071, 2.3157, 2.2498, 2.2387,\n        2.1295, 2.6759, 1.9556, 2.2247, 2.1898, 2.1992, 2.3438, 2.1877, 2.2954,\n        2.1327], grad_fn=<NegBackward>)\n\nLoss: tensor(2.4044, grad_fn=<NegBackward>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Function to calculate accuracy\n\nLet’s also implement a function to calculate the accuracy of our model. For each prediction, if the index with the largest value matches the target value, then the prediction was correct.","metadata":{}},{"cell_type":"code","source":"def accuracy(out, yb):\n    preds = torch.argmax(out, dim=1)\n    return (preds == yb).float().mean()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.685214Z","iopub.execute_input":"2021-08-09T13:41:32.685651Z","iopub.status.idle":"2021-08-09T13:41:32.692557Z","shell.execute_reply.started":"2021-08-09T13:41:32.685606Z","shell.execute_reply":"2021-08-09T13:41:32.691571Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Let's check the accuracy of our random model.\n# We do not expect good result since we have not done any training yet.\n\nprint(\"Accuracy:\", accuracy(preds, yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.694030Z","iopub.execute_input":"2021-08-09T13:41:32.694471Z","iopub.status.idle":"2021-08-09T13:41:32.712503Z","shell.execute_reply.started":"2021-08-09T13:41:32.694424Z","shell.execute_reply":"2021-08-09T13:41:32.711300Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy: tensor(0.0312)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training Loop\n\nWe can now run a training loop. For each iteration, we will:\n\n- select a mini-batch of data (of size `bs`)\n- use the model to make predictions\n- calculate the loss\n- `loss.backward()` updates the gradients of the model, in this case, weights and bias.\n\nWe now use these gradients to update the weights and bias. We do this within the `torch.no_grad()` context manager, because we do not want these actions to be recorded for our next calculation of the gradient. You can read more about how PyTorch’s [Autograd](https://pytorch.org/docs/stable/notes/autograd.html) records operations here.\n\nWe then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. `loss.backward()` adds the gradients to whatever is already stored, rather than replacing them).","metadata":{}},{"cell_type":"code","source":"from IPython.core.debugger import set_trace\n\nlr = 0.5     # learning rate\nepochs = 2   # how many epochs to train for\n\nfor epoch in range(epochs):\n    \n    # Loop 782 rounds for 50k of data, each time with 64 records in a batch.\n    for i in range((n - 1) // bs + 1): \n        #   set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()                   # To backpropagate the error (gradients are computed).\n        \n        with torch.no_grad():\n            \n            weights -= weights.grad * lr  # Update weights and bias.\n            bias -= bias.grad * lr           \n            \n            weights.grad.zero_()          # Zero the gradients.\n            bias.grad.zero_()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:32.714429Z","iopub.execute_input":"2021-08-09T13:41:32.714778Z","iopub.status.idle":"2021-08-09T13:41:33.815420Z","shell.execute_reply.started":"2021-08-09T13:41:32.714747Z","shell.execute_reply":"2021-08-09T13:41:33.814574Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Let’s check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have decreased and accuracy to have increased, and they have.","metadata":{}},{"cell_type":"code","source":"print(\"Loss    :\", loss_func(model(xb), yb)) \nprint(\"Accuracy:\", accuracy(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:33.816590Z","iopub.execute_input":"2021-08-09T13:41:33.817037Z","iopub.status.idle":"2021-08-09T13:41:33.826150Z","shell.execute_reply.started":"2021-08-09T13:41:33.816996Z","shell.execute_reply":"2021-08-09T13:41:33.824747Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Loss    : tensor(0.0805, grad_fn=<NegBackward>)\nAccuracy: tensor(1.)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Using `torch.nn.functional`\n\nWe will now refactor our code, so that it does the same thing as before, only we’ll start taking advantage of PyTorch’s nn classes to make it more concise and flexible.\n\nWe will be replacing our hand-written activation and loss functions with those from `torch.nn.functional` (which is generally imported into the namespace `F` by convention). This module contains all the functions in the `torch.nn` library.\n\n\n\n**If we're using negative log likelihood loss and log softmax activation, then Pytorch provides a single function [F.cross_entropy](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) that combines the two. So we can even remove the activation function from our model.**","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# loss_func = nll            # Previously we use nll loss function.\nloss_func = F.cross_entropy  # Now we use cross_entropy (that combines log_softmax and nll loss).\n\n\ndef model(xb):    \n    # return log_softmax(xb @ weights + bias)    # Previous model need to call log_softmax.\n    return xb @ weights + bias                   # Now we don't need to call log_softmax.   ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:33.828461Z","iopub.execute_input":"2021-08-09T13:41:33.828894Z","iopub.status.idle":"2021-08-09T13:41:33.839751Z","shell.execute_reply.started":"2021-08-09T13:41:33.828856Z","shell.execute_reply":"2021-08-09T13:41:33.838890Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Note that we no longer call `log_softmax` in the model function. Let’s confirm that our loss and accuracy are the same as before:","metadata":{}},{"cell_type":"code","source":"print(\"Loss    :\", loss_func(model(xb), yb))\nprint(\"Accuracy:\", accuracy(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:33.840947Z","iopub.execute_input":"2021-08-09T13:41:33.841350Z","iopub.status.idle":"2021-08-09T13:41:33.867957Z","shell.execute_reply.started":"2021-08-09T13:41:33.841319Z","shell.execute_reply":"2021-08-09T13:41:33.866925Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Loss    : tensor(0.0805, grad_fn=<NllLossBackward>)\nAccuracy: tensor(1.)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. Refactor using `nn.Module`\n\nNext up, we’ll use `nn.Module` and `nn.Parameter`, for a clearer and more concise training loop. We subclass `nn.Module` (which itself is a class and able to keep track of state). \n\nIn this case, we want to create a class that holds our weights, bias, and method for the forward step. \n\n`nn.Module` has a number of attributes and methods (such as `.parameters()` and `.zero_grad()`) which we will be using.\n\nNote that `nn.Module` objects are used as if they are functions (i.e. they are callable), but behind the scenes Pytorch will call our forward method automatically.","metadata":{}},{"cell_type":"code","source":"from torch import nn\n\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        self.bias = nn.Parameter(torch.zeros(10))\n\n    def forward(self, xb):\n        return xb @ self.weights + self.bias","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:33.869082Z","iopub.execute_input":"2021-08-09T13:41:33.869502Z","iopub.status.idle":"2021-08-09T13:41:33.876069Z","shell.execute_reply.started":"2021-08-09T13:41:33.869469Z","shell.execute_reply":"2021-08-09T13:41:33.874917Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Instantiate our model\n\nmodel = Mnist_Logistic()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:33.882024Z","iopub.execute_input":"2021-08-09T13:41:33.882631Z","iopub.status.idle":"2021-08-09T13:41:33.891900Z","shell.execute_reply.started":"2021-08-09T13:41:33.882594Z","shell.execute_reply":"2021-08-09T13:41:33.890819Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(\"Loss:\", loss_func(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:33.895270Z","iopub.execute_input":"2021-08-09T13:41:33.895902Z","iopub.status.idle":"2021-08-09T13:41:33.908863Z","shell.execute_reply.started":"2021-08-09T13:41:33.895849Z","shell.execute_reply":"2021-08-09T13:41:33.907383Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Loss: tensor(2.4549, grad_fn=<NllLossBackward>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We’ll wrap our training loop in a `fit` function so we can run it again later.","metadata":{}},{"cell_type":"code","source":"def fit():\n    for epoch in range(epochs):\n        for i in range((n - 1) // bs + 1):\n            start_i = i * bs\n            end_i = start_i + bs\n            xb = x_train[start_i:end_i]\n            yb = y_train[start_i:end_i]\n            pred = model(xb)\n            loss = loss_func(pred, yb)\n\n            loss.backward()       # To backpropagate the error (gradients are computed).\n            \n            with torch.no_grad():\n                # Now we take advantage of model.parameters() and model.zero_grad()\n                for p in model.parameters():\n                    p -= p.grad * lr\n                model.zero_grad()\n                \n            '''\n            Below is the previous chunk of codes where we had to\n            update the values for each parameter by name, \n            and manually zero out the grads for each parameter separately.  \n            '''\n            #with torch.no_grad():\n                \n                #weights -= weights.grad * lr  # Update weights and bias.\n                #bias -= bias.grad * lr\n                            \n                #weights.grad.zero_()          # Zero the gradients.\n                #bias.grad.zero_()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:33.911347Z","iopub.execute_input":"2021-08-09T13:41:33.911962Z","iopub.status.idle":"2021-08-09T13:41:33.921377Z","shell.execute_reply.started":"2021-08-09T13:41:33.911890Z","shell.execute_reply":"2021-08-09T13:41:33.920232Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"fit()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:33.922933Z","iopub.execute_input":"2021-08-09T13:41:33.923323Z","iopub.status.idle":"2021-08-09T13:41:34.971022Z","shell.execute_reply.started":"2021-08-09T13:41:33.923238Z","shell.execute_reply":"2021-08-09T13:41:34.969970Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Double-check that our loss has gone down.\n\nprint(\"Loss:\", loss_func(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:34.972246Z","iopub.execute_input":"2021-08-09T13:41:34.972604Z","iopub.status.idle":"2021-08-09T13:41:34.980465Z","shell.execute_reply.started":"2021-08-09T13:41:34.972570Z","shell.execute_reply":"2021-08-09T13:41:34.979263Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Loss: tensor(0.0825, grad_fn=<NllLossBackward>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Refactor using `nn.Linear`\n\nInstead of manually defining and initializing `self.weights` and `self.bias`, and calculating `xb  @ self.weights + self.bias`, we will instead use the Pytorch class `nn.Linear` for a linear layer, which does all that for us. Pytorch has many types of predefined layers that can greatly simplify our code, and often makes it faster too.","metadata":{}},{"cell_type":"code","source":"class Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(784, 10)\n        #self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        #self.bias = nn.Parameter(torch.zeros(10))\n\n    def forward(self, xb):\n        return self.lin(xb)\n        #return xb @ self.weights + self.bias","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:34.981748Z","iopub.execute_input":"2021-08-09T13:41:34.982139Z","iopub.status.idle":"2021-08-09T13:41:34.991912Z","shell.execute_reply.started":"2021-08-09T13:41:34.982104Z","shell.execute_reply":"2021-08-09T13:41:34.990758Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Instantiate our model and calculate the loss.\n\nmodel = Mnist_Logistic()\nprint(\"Loss:\", loss_func(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:34.993373Z","iopub.execute_input":"2021-08-09T13:41:34.993771Z","iopub.status.idle":"2021-08-09T13:41:35.013416Z","shell.execute_reply.started":"2021-08-09T13:41:34.993737Z","shell.execute_reply":"2021-08-09T13:41:35.012490Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Loss: tensor(2.2629, grad_fn=<NllLossBackward>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Call fit to train our model and calculate the loss again.\n\nfit()\nprint(\"Loss:\", loss_func(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:35.015839Z","iopub.execute_input":"2021-08-09T13:41:35.016244Z","iopub.status.idle":"2021-08-09T13:41:36.091118Z","shell.execute_reply.started":"2021-08-09T13:41:35.016204Z","shell.execute_reply":"2021-08-09T13:41:36.087135Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Loss: tensor(0.0806, grad_fn=<NllLossBackward>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5. Refactor using `optim`\n\nPytorch also has a package with various optimization algorithms, `torch.optim`. We can use the `step` method from our optimizer to take a forward step, instead of manually updating each parameter.\n\n> `opt.step()`\n>\n> `opt.zero_grad()`","metadata":{}},{"cell_type":"code","source":"from torch import optim\n\n# Define a function to create our model and optimizer so we can reuse it in the future.\n\ndef get_model():\n    model = Mnist_Logistic()\n    return model, optim.SGD(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:36.092321Z","iopub.execute_input":"2021-08-09T13:41:36.092600Z","iopub.status.idle":"2021-08-09T13:41:36.099810Z","shell.execute_reply.started":"2021-08-09T13:41:36.092573Z","shell.execute_reply":"2021-08-09T13:41:36.098775Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Get model and print loss\n\nmodel, opt = get_model()\nprint(\"Loss:\", loss_func(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:36.101280Z","iopub.execute_input":"2021-08-09T13:41:36.101715Z","iopub.status.idle":"2021-08-09T13:41:36.118974Z","shell.execute_reply.started":"2021-08-09T13:41:36.101669Z","shell.execute_reply":"2021-08-09T13:41:36.117768Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Loss: tensor(2.2898, grad_fn=<NllLossBackward>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train model and print loss again.\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()   # To backpropagate the error (gradients are computed).\n\n        opt.step()        # Use optim to update parameters based on current gradients.\n        opt.zero_grad()   # Use optim to zero out the gradients.\n\n        '''\n        Below is the previous chunk of codes where we had to use\n        model.parameters() and model.zero_grad() \n        to update and zero out the grads.  \n        '''\n        #with torch.no_grad():            \n            #for p in model.parameters():\n            #    p -= p.grad * lr\n            #model.zero_grad()\n                \nprint(\"Loss:\", loss_func(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:36.120619Z","iopub.execute_input":"2021-08-09T13:41:36.121016Z","iopub.status.idle":"2021-08-09T13:41:36.998138Z","shell.execute_reply.started":"2021-08-09T13:41:36.120907Z","shell.execute_reply":"2021-08-09T13:41:36.996796Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Loss: tensor(0.0810, grad_fn=<NllLossBackward>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 6. Refactor using `Dataset`\n\nPyTorch has an abstract `Dataset` class. A `Dataset` can be anything that has a `__len__` function (called by Python’s standard `len` function) and a `__getitem__` function as a way of indexing into it.\n\nPyTorch’s `TensorDataset` is a `Dataset` wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\n\n# We will combine both x_train and y_train in a single TensorDataset, \n# which will be easier to iterate over and slice.\n\ntrain_ds = TensorDataset(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:36.999541Z","iopub.execute_input":"2021-08-09T13:41:37.000026Z","iopub.status.idle":"2021-08-09T13:41:37.005737Z","shell.execute_reply.started":"2021-08-09T13:41:36.999990Z","shell.execute_reply":"2021-08-09T13:41:37.004827Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model, opt = get_model()\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n               \n        '''\n        Previously we need to \n        iterate through minibatches of x and y values separately.\n        '''        \n        #start_i = i * bs\n        #end_i = start_i + bs\n        #xb = x_train[start_i:end_i]\n        #yb = y_train[start_i:end_i]\n        \n        '''Now, we can do these two steps together in one line'''\n        xb, yb = train_ds[i*bs : i*bs+bs]\n        \n        pred = model(xb)\n        loss = loss_func(pred, yb)       \n\n        loss.backward()   # To backpropagate the error (gradients are computed).\n        opt.step()        # To update parameters based on current gradients.\n        opt.zero_grad()   # To zero out the gradients.\n\nprint(\"Loss:\", loss_func(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:37.007209Z","iopub.execute_input":"2021-08-09T13:41:37.007709Z","iopub.status.idle":"2021-08-09T13:41:37.894115Z","shell.execute_reply.started":"2021-08-09T13:41:37.007672Z","shell.execute_reply":"2021-08-09T13:41:37.893181Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Loss: tensor(0.0810, grad_fn=<NllLossBackward>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 7. Refactor using `DataLoader`\n\nPytorch’s `DataLoader` is responsible for managing batches. You can create a `DataLoader` from any `Dataset`. \n\n`DataLoader` makes it easier to iterate over batches. Rather than having to use `train_ds[i*bs : i*bs+bs]`, the `DataLoader` gives us each minibatch automatically.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:37.895225Z","iopub.execute_input":"2021-08-09T13:41:37.895694Z","iopub.status.idle":"2021-08-09T13:41:37.899803Z","shell.execute_reply.started":"2021-08-09T13:41:37.895648Z","shell.execute_reply":"2021-08-09T13:41:37.899064Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model, opt = get_model()\n\nfor epoch in range(epochs):\n    '''\n    Previously, our loop iterated over batches (xb, yb).\n    '''    \n    #for i in range((n - 1) // bs + 1):\n    #    xb, yb = train_ds[i*bs : i*bs+bs]\n    '''\n    Now, our loop is much cleaner, \n    as (xb, yb) are loaded automatically from the data loader.\n    '''\n    for xb, yb in train_dl:\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()  # To backpropagate the error (gradients are computed).\n        opt.step()       # To update parameters based on current gradients.\n        opt.zero_grad()  # To zero out the gradients.\n\nprint(\"Loss:\", loss_func(model(xb), yb))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:37.901035Z","iopub.execute_input":"2021-08-09T13:41:37.901499Z","iopub.status.idle":"2021-08-09T13:41:40.142352Z","shell.execute_reply.started":"2021-08-09T13:41:37.901460Z","shell.execute_reply":"2021-08-09T13:41:40.141140Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Loss: tensor(0.0832, grad_fn=<NllLossBackward>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Thanks to Pytorch’s `nn.Module`, `nn.Parameter`, `Dataset`, and `DataLoader`, our training loop is now dramatically smaller and easier to understand. Let’s now try to add the basic features necessary to create effective models in practice.","metadata":{}},{"cell_type":"markdown","source":"# 8. Add Validation Set\n\nAbove, we were just trying to get a reasonable training loop set up for use on our training data. In reality, we always should also have a validation set, in order to identify if you are overfitting.\n\nNotes:\n\n1. Shuffling the training data is important to prevent correlation between batches and overfitting. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n\n2. We’ll use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly.","metadata":{}},{"cell_type":"code","source":"train_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n\nvalid_ds = TensorDataset(x_valid, y_valid)\nvalid_dl = DataLoader(valid_ds, batch_size=bs*2) # No need shuffle, double the batch size.","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:40.144165Z","iopub.execute_input":"2021-08-09T13:41:40.144588Z","iopub.status.idle":"2021-08-09T13:41:40.151352Z","shell.execute_reply.started":"2021-08-09T13:41:40.144543Z","shell.execute_reply":"2021-08-09T13:41:40.150068Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model, opt = get_model()\n\nfor epoch in range(epochs):\n    model.train()                   # See note below.\n    for xb, yb in train_dl:\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()  # To backpropagate the error (gradients are computed).\n        opt.step()       # To update parameters based on current gradients.\n        opt.zero_grad()  # To zero out the gradients.\n\n    model.eval()                    # See note below.\n    \n    # Calculate and print the validation loss at the end of each epoch.\n    with torch.no_grad():\n        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n    \n    validation_loss = valid_loss / len(valid_dl)\n    print(\"Epoch\", epoch, \"Validation loss\", validation_loss.item())\n    \n# Note:\n# We always call model.train() before training, and model.eval() before inference, \n# because these are used by layers such as nn.BatchNorm2d and nn.Dropout \n# to ensure appropriate behaviour for these different phases.","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:40.153050Z","iopub.execute_input":"2021-08-09T13:41:40.153492Z","iopub.status.idle":"2021-08-09T13:41:42.669593Z","shell.execute_reply.started":"2021-08-09T13:41:40.153456Z","shell.execute_reply":"2021-08-09T13:41:42.668496Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch 0 Validation loss 0.3051762878894806\nEpoch 1 Validation loss 0.29607948660850525\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 9. Create `fit()` and `get_data()`","metadata":{}},{"cell_type":"code","source":"# loss_batch() computes the loss for one batch.\n# To be used for both the training set and the validation set.\n\ndef loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    # Only pass in an optimizer for the training set to perform backprop.\n    # We don't need to do this for the validation set.\n    if opt is not None:\n        loss.backward()  # To backpropagate the error (gradients are computed).\n        opt.step()       # To update parameters based on current gradients.\n        opt.zero_grad()  # To zero out the gradients.\n\n    return loss.item(), len(xb)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:42.670883Z","iopub.execute_input":"2021-08-09T13:41:42.671166Z","iopub.status.idle":"2021-08-09T13:41:42.679514Z","shell.execute_reply.started":"2021-08-09T13:41:42.671139Z","shell.execute_reply":"2021-08-09T13:41:42.678172Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# fit() runs the necessary operations to train our model and \n# computes the training and validation losses for each epoch.\n\nimport numpy as np\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        \n        # Calculate and print the validation loss at the end of each epoch.\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(\"Epoch\", epoch, \"Validation loss\", val_loss)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:42.680749Z","iopub.execute_input":"2021-08-09T13:41:42.681085Z","iopub.status.idle":"2021-08-09T13:41:42.690995Z","shell.execute_reply.started":"2021-08-09T13:41:42.681053Z","shell.execute_reply":"2021-08-09T13:41:42.689649Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# get_data() returns dataloaders for the training and validation sets.\n\ndef get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs*2), # No need shuffle, double the batch size.\n    )","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:42.693466Z","iopub.execute_input":"2021-08-09T13:41:42.694307Z","iopub.status.idle":"2021-08-09T13:41:42.707703Z","shell.execute_reply.started":"2021-08-09T13:41:42.694254Z","shell.execute_reply":"2021-08-09T13:41:42.706764Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code by calling `get_data`, `get_model` and `fit`:","metadata":{}},{"cell_type":"code","source":"train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\nmodel, opt = get_model()\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:42.709276Z","iopub.execute_input":"2021-08-09T13:41:42.710007Z","iopub.status.idle":"2021-08-09T13:41:45.208058Z","shell.execute_reply.started":"2021-08-09T13:41:42.709960Z","shell.execute_reply":"2021-08-09T13:41:45.207177Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 0 Validation loss 0.31352322623729706\nEpoch 1 Validation loss 0.28131680501699446\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 10. Switch to CNN\n\nWe are now going to build our neural network with 3 convolutional layers. Because none of the functions in the previous section assume anything about the model form, we’ll be able to use them to train a CNN without any modification.\n\nWe will use Pytorch’s predefined [`Conv2d`](https://pytorch.org/docs/stable/nn.html#convolution-layers) class as our convolutional layer. We define a CNN with 3 convolutional layers. Each convolution is followed by a ReLU. At the end, we perform an average pooling. (Note that `view` is PyTorch’s version of numpy’s `reshape`)","metadata":{}},{"cell_type":"code","source":"class Mnist_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, xb):\n        xb = xb.view(-1, 1, 28, 28)\n        xb = F.relu(self.conv1(xb))\n        xb = F.relu(self.conv2(xb))\n        xb = F.relu(self.conv3(xb))\n        xb = F.avg_pool2d(xb, 4)\n        return xb.view(-1, xb.size(1))\n\nlr = 0.1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:45.209522Z","iopub.execute_input":"2021-08-09T13:41:45.209888Z","iopub.status.idle":"2021-08-09T13:41:45.219915Z","shell.execute_reply.started":"2021-08-09T13:41:45.209851Z","shell.execute_reply":"2021-08-09T13:41:45.218924Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"[Momentum](https://cs231n.github.io/neural-networks-3/#sgd) is a variation on stochastic gradient descent (SGD) that takes previous updates into account as well and generally leads to faster training.","metadata":{}},{"cell_type":"code","source":"model = Mnist_CNN()\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:45.221096Z","iopub.execute_input":"2021-08-09T13:41:45.221615Z","iopub.status.idle":"2021-08-09T13:41:53.297761Z","shell.execute_reply.started":"2021-08-09T13:41:45.221582Z","shell.execute_reply":"2021-08-09T13:41:53.296519Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Epoch 0 Validation loss 0.6487905010700226\nEpoch 1 Validation loss 0.5284168268680572\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 11. `nn.Sequential`\n\nA [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) object runs each of the modules contained within it, in a sequential manner. This is a simpler way of writing our neural network.\n\nTo take advantage of this, we need to be able to easily define a custom layer from a given function. For instance, PyTorch doesn’t have a view layer, and we need to create one for our network. `Lambda` will create a layer that we can then use when defining a network with `Sequential`.","metadata":{}},{"cell_type":"code","source":"class Lambda(nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 1, 28, 28)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:53.298938Z","iopub.execute_input":"2021-08-09T13:41:53.299229Z","iopub.status.idle":"2021-08-09T13:41:53.306213Z","shell.execute_reply.started":"2021-08-09T13:41:53.299202Z","shell.execute_reply":"2021-08-09T13:41:53.304961Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model = nn.Sequential(\n    Lambda(preprocess),\n    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.AvgPool2d(4), # kernel size is 4x4\n    Lambda(lambda x: x.view(x.size(0), -1)),\n)\n\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:41:53.307742Z","iopub.execute_input":"2021-08-09T13:41:53.308092Z","iopub.status.idle":"2021-08-09T13:42:01.523184Z","shell.execute_reply.started":"2021-08-09T13:41:53.308060Z","shell.execute_reply":"2021-08-09T13:42:01.522420Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Epoch 0 Validation loss 0.39112355480194094\nEpoch 1 Validation loss 0.31412444493770597\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 12. Wrapping DataLoader\n\nOur CNN is fairly concise, but it only works with MNIST, because:\n- It assumes the input is a 28*28 long vector\n- It assumes that the final CNN grid size is 4*4 (since that’s the average pooling kernel size we used)\n\nLet’s get rid of these two assumptions, so our model works with any 2d single channel image. First, we can remove the initial Lambda layer by moving the data preprocessing into a generator:","metadata":{}},{"cell_type":"code","source":"def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:42:01.524215Z","iopub.execute_input":"2021-08-09T13:42:01.524630Z","iopub.status.idle":"2021-08-09T13:42:01.531631Z","shell.execute_reply.started":"2021-08-09T13:42:01.524600Z","shell.execute_reply":"2021-08-09T13:42:01.530707Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"Next, we can replace [`nn.AvgPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d) with [`nn.AdaptiveAvgPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d), which allows us to define the size of the output tensor we want, rather than the input tensor we have. As a result, our model will work with any size input.","metadata":{}},{"cell_type":"code","source":"model = nn.Sequential(\n    #Lambda(preprocess),     # Remove this Lambda layer.\n    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    #nn.AvgPool2d(4),        # kernel size is 4x4\n    nn.AdaptiveAvgPool2d(1), # output size of 1x1\n    Lambda(lambda x: x.view(x.size(0), -1)),    \n)\n\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:42:01.532708Z","iopub.execute_input":"2021-08-09T13:42:01.533148Z","iopub.status.idle":"2021-08-09T13:42:01.547325Z","shell.execute_reply.started":"2021-08-09T13:42:01.533118Z","shell.execute_reply":"2021-08-09T13:42:01.546492Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"fit(epochs, model, loss_func, opt, train_dl, valid_dl)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:42:01.548685Z","iopub.execute_input":"2021-08-09T13:42:01.549246Z","iopub.status.idle":"2021-08-09T13:42:10.181882Z","shell.execute_reply.started":"2021-08-09T13:42:01.549207Z","shell.execute_reply":"2021-08-09T13:42:10.180679Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Epoch 0 Validation loss 0.36453516092300414\nEpoch 1 Validation loss 0.27074480847120286\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Using your GPU\n\nFirst, check that your GPU is working in Pytorch.","metadata":{}},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:42:10.183515Z","iopub.execute_input":"2021-08-09T13:42:10.183955Z","iopub.status.idle":"2021-08-09T13:42:10.189353Z","shell.execute_reply.started":"2021-08-09T13:42:10.183909Z","shell.execute_reply":"2021-08-09T13:42:10.188236Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"False\n","output_type":"stream"}]},{"cell_type":"code","source":"# If GPU is available, create a device object for it.\n\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndev","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:42:10.190696Z","iopub.execute_input":"2021-08-09T13:42:10.191054Z","iopub.status.idle":"2021-08-09T13:42:10.204724Z","shell.execute_reply.started":"2021-08-09T13:42:10.191023Z","shell.execute_reply":"2021-08-09T13:42:10.203944Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"# Let’s update preprocess to move batches to the GPU.\n\ndef preprocess(x, y):    \n    #return x.view(-1, 1, 28, 28), y                  # Previous code\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)   # Move batches to GPU\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:42:10.206099Z","iopub.execute_input":"2021-08-09T13:42:10.206383Z","iopub.status.idle":"2021-08-09T13:42:10.216389Z","shell.execute_reply.started":"2021-08-09T13:42:10.206356Z","shell.execute_reply":"2021-08-09T13:42:10.215591Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Finally, we can move our model to the GPU.\n\nmodel.to(dev)  \nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:42:10.217648Z","iopub.execute_input":"2021-08-09T13:42:10.218117Z","iopub.status.idle":"2021-08-09T13:42:10.239348Z","shell.execute_reply.started":"2021-08-09T13:42:10.218084Z","shell.execute_reply":"2021-08-09T13:42:10.238011Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Fit shoud run faster now on GPU.\n\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T13:42:10.241076Z","iopub.execute_input":"2021-08-09T13:42:10.241555Z","iopub.status.idle":"2021-08-09T13:42:18.482693Z","shell.execute_reply.started":"2021-08-09T13:42:10.241434Z","shell.execute_reply":"2021-08-09T13:42:18.481588Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Epoch 0 Validation loss 0.19970220215320586\nEpoch 1 Validation loss 0.1618137788295746\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Summary\n\n`torch.nn`\n- `Module`: creates a callable which behaves like a function, but can also contain state(such as neural net layer weights). It knows what Parameter (s) it contains and can zero all their gradients, loop through them for weight updates, etc.\n- `Parameter`: a wrapper for a tensor that tells a Module that it has weights that need updating during backprop. Only tensors with the `requires_grad` attribute set are updated.\n- `functional`: a module (usually imported into the `F` namespace by convention) which contains activation functions, loss functions, etc, as well as non-stateful versions of layers such as convolutional and linear layers.\n\n`torch.optim`: Contains optimizers such as `SGD`, which update the weights of Parameter during the backward step.\n\n`Dataset`: An abstract interface of objects with a `__len__` and a `__getitem__`, including classes provided with Pytorch such as `TensorDataset`.\n\n`DataLoader`: Takes any `Dataset` and creates an iterator which returns batches of data.","metadata":{}}]}